{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datenbeschaffung\n",
    "In typischen Umgebungen sind Daten in einer relationalen Datenbank verfügbar (oder andere zentrale Datenspeicher) und über mehrere Tabellen/Dokumente/Dateien verteilt. Um darauf zuzugreifen, müssen Sie zunächst Ihre Anmeldedaten und Zugriffsberechtigungen erhalten. Machen Sie sich früh mit dem Datenschema vertraut. \n",
    "In diesem Projekt sind jedoch viele Dinge Einfacher: Sie laden einfach eine einzige komprimierte Datei, housing.tgz, herunter, die eine CSV-Datei (comma-separated value) namens housing.csv mit allen Daten enthält.\n",
    "Sie können sie mit Ihrem Webbrowser herunterladen und tar xzf housing.tgz ausführen, um die Datei zu dekomprimieren und die CSV-Datei zu extrahieren. Besser ist es aber, eine kleine Python-Funktion zu erstellen, um das zu tun. Das ist insbesondere dann nützlich, wenn sich die Daten regelmäßig ändern, da eine eigene Funktion Ihnen ermöglicht, ein kleines Skript zu schreiben, das Sie immer dann ausführen können, wenn Sie die neuesten Daten abrufen müssen. Den Prozess der Datenbeschaffung zu automatisieren ist auch nützlich, wenn Sie die Datensätze auf mehreren Maschinen installieren müssen. \n",
    "\n",
    "Folgender Code läd eine gepackte Datei aus dem Internet, erstellt lokal ein Verzeichnis, entpackt die Datei und kopiert den Inhalt des Archivs in dieses Verzeichnis. Wenn alles geklappt hat, sehen Sie in Ihrem Python Arbeitsverzeichnis ein Unterverzeichnis *datasets/housing* mit den beiden Dateien *housing.tgz* und *housing.csv*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übung 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verzeichnis und Datei schon vorhanden!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
    "HOUSING_PATH = \"datasets/housing\"\n",
    "HOUSING_URL = DOWNLOAD_ROOT + HOUSING_PATH + \"/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "        tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "        urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "        housing_tgz = tarfile.open(tgz_path)\n",
    "        housing_tgz.extractall(path=housing_path)\n",
    "        housing_tgz.close()\n",
    "        print(\"Datei erforlgeich entpackt!\")\n",
    "    else:\n",
    "        print(\"Verzeichnis und Datei schon vorhanden!\")\n",
    "        \n",
    "fetch_housing_data() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliotheken zur Datenaufbereitung\n",
    "Um die Daten weiter zu bearbeiten und anzeigen zu lassen, benötigen Sie eine Reihe von Python-Bibliotheken, die im folgenden kurz vorgestellt werden:\n",
    "\n",
    "**NumPy:**\n",
    "Programmbibliothek, die eine einfache Handhabung von Vektoren, Matrizen oder generell großen mehrdimensionalen Arrays ermöglicht. Neben den Datenstrukturen bietet NumPy auch effizient implementierte Funktionen für numerische Berechnungen an\n",
    "\n",
    "**Pandas:**\n",
    "Bibliothek, die Datenstrukturen und Operatoren für den Zugriff auf numerische Tabellen und Zeitreihen enthält\n",
    "\n",
    "**Matplotlib:**\n",
    "Erlaubt die einfache Anfertigung von mathematischen Grafiken aller Art\n",
    "\n",
    "**Scikit-Learn:**\n",
    "Programmbibliothek, die verschiedene Klassifikations-, Regressions- und Clustering-Algorithmen, darunter Support-Vektor-Maschinen, Random Forest, Gradient Boosting, k-means und DBSCAN, anbietet. Sie ist so konzipiert, dass sie mit den numerischen und wissenschaftlichen Python-Bibliotheken NumPy und SciPy zusammenarbeitet. \n",
    "\n",
    "Die Module können in der EIngabeaufforderung durch den Befehl:<br>\n",
    "*pip install matplotlib, numpy, pandas, scipy, sklearn* <br>\n",
    "nachgeladen werden, falls sie noch nicht vorhanden sind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übung 2\n",
    "Überführen Sie die Daten in der csv-Datei in ein strukturiertes Datenformat mit der Pandas Bibliothek. Dafür importieren Sie die pandas Bibliothek und legen eine eigene Funktion zum Einlesen der Daten an. \n",
    "\n",
    "- Einlesen von Daten aus einer csv-Datei mit read() Methode, Bsp.: read(csv Dateipfad)\n",
    "- Ausgeben der top 5 Datensätze mit der Methode head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data():\n",
    "    csv_path = os.path.join(\"datasets/housing/housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jede Zeile repräsentiert einen Distrikt. Es gibt 10 Attribute: Längengrad, Breitengrad, Wohnungsmittelwert, Gesamtzahl der Zimmer, Gesamtzahl der Schlafzimmer, Bevölkerung, Haushalte, Medianeinkommen, Medianwert des Hauses und Ozean_Nähe. \n",
    "\n",
    "Die **info()-Methode** ist nützlich, um eine schnelle Beschreibung der Daten zu erhalten, insbesondere die Gesamtzahl der Zeilen und den Typ und die Anzahl der Nicht-Null-Werte jedes Attributs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übung 3\n",
    "Eine weitere schnelle Möglichkeit, ein Gefühl für die Art der Daten zu bekommen, ist die Erstellung eines Histogramms für jedes numerische Attribut. Ein Histogramm zeigt die Anzahl der Instanzen (auf der vertikalen Achse), die einen bestimmten Wertebereich (auf der horizontalen Achse) haben. \n",
    "Um Daten schnell zu visualisieren, können Sie die **Matplotlib-Bibliothek** nutzen. Matplotlib ist eng mit Pandas integriert. Ein Datenobjekt in Pandas kann direkt mit Matplotlib angezeigt werden. Auf dem Pandas-Objekt können Sie die **hist()-Methode** aufrufen, um ein Histogramm für jedes numerische Attribut zu erstellen. Die hist-Methode erwartet zwei Paramter: *bins* für die Granularität und *figsize* für die Größe der Abbildungen.\n",
    "\n",
    "\n",
    "Angezeigt wird es damit aber noch nicht. Erst wenn Sie die show()-Methode aus der Matplotlib-Bibliothek verwenden, werden die Histogramme auch angezeigt.\n",
    "\n",
    "Aufgabe 1: Erstellen Sie in ihrem Pandas-Datenobjekt die Histogramme, bins = 30, figsize = (20,15).\n",
    "Aufgabe 2: Geben Sie die Histogramme aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erstellen von Test- und Trainingsdatensätzen\n",
    "Testdatensätze werden verwendet, um die Performance eines KI-Verfahrens zu überpüfen. Mit den Trainingsdaten wird ein Algorithmus trainiert. Die erset Aufgabe nahc der Datenbeschaffung und Sichtung ist also das Aufteilen der Daten in Test- und Trainingsdaten.\n",
    "\n",
    "Bisher haben Sie nur einen kurzen Blick auf die Daten geworfen, und sicherlich sollten Sie noch eine ganze Menge mehr darüber lernen, bevor Sie sich entscheiden, welche Algorithmen Sie verwenden. Wenn Sie sich die Testmenge ansehen, könnten Sie auf ein scheinbar interessantes Muster in den Testdaten stoßen, das Sie dazu bringt, eine bestimmte Art von Modell für maschinelles Lernen auszuwählen. Wenn Sie den Generalisierungsfehler mit Hilfe des Tests abschätzen, wird Ihre Schätzung zu optimistisch sein und Sie werden ein System einführen, das nicht so gut abschneiden wird wie erwartet. Dies wird als *Datenschnüffel-Verzerrung* bezeichnet. Das Erstellen eines Testsatzes ist theoretisch recht einfach: Sie müssen nur einige Instanzen nach dem Zufallsprinzip auswählen, typischerweise 20% des Datensatzes und ihn beiseite legen. Entscheidend ist dabei wie die Daten aus der Grundmenge ausgewählt werden, auch wenn sich diese Grundmenge über die Zeit verändert (weil neue Daten hinzukommen). \n",
    "\n",
    "Nutzen Sie für die Test- und Trainsdatenerstellung die Methode train_test_split aus der Bibliothek sklearn.model_selection Die Methode hat drei Paramter:\n",
    "- Pandas-datenobjekt\n",
    "- test_size (Anzahl Testdaten in Prozent vom ursprünglichen Datensatz (z.B. 0.2)\n",
    "- random_state= Parameter für Zufallszahlengenerator (z. B. 10)\n",
    "\n",
    "Aufgabe 1: Erstellen Sie einen Trainings- und Testdatensatz mit test_size = 0.2 und random_state = 42.<br>\n",
    "Aufgabe 2: Erstellen Sie einen Plot aller Attribute im Trains- und Testdatensatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bislang haben wir rein zufällige Stichprobenverfahren in Betracht gezogen. Dies ist in der Regel in Ordnung, wenn Ihr Datensatz groß genug ist (insbesondere im Verhältnis zur Anzahl der Attribute). Aber wenn nicht, besteht die Gefahr, dass Sie eine signifikante Stichproben-Verzerrung einführen. Wenn ein Umfrageunternehmen beschließt, 1.000 Personen anzurufen, um ihnen ein paar Fragen zu stellen, wählt es nicht einfach 1.000 Personen nach dem Zufallsprinzip in einer Telefonzelle aus. Sie versuchen sicherzustellen, dass diese 1.000 Personen repräsentativ für die gesamte Bevölkerung sind. Die US-Bevölkerung setzt sich zum Beispiel aus 51,3 % Frauen und 48,7 % Männern zusammen, so dass eine gut durchgeführte Umfrage in den USA versuchen würde, dieses Verhältnis in der Stichprobe beizubehalten: 513 weibliche und 487 männliche Personen. Dies wird als **stratifizierte Stichprobenziehung** bezeichnet: Die Bevölkerung wird in homogene Untergruppen, die als Schichten bezeichnet werden, unterteilt, und aus jeder Schicht wird die richtige Anzahl von Instanzen gezogen, um zu gewährleisten, dass der Testsatz für die Gesamtbevölkerung repräsentativ ist. \n",
    "\n",
    "Nehmen wir an, Sie haben mit Experten gesprochen, die Ihnen sagen, dass der Medianwert des Einkommens ein sehr wichtige Eigenschaft zur Vorhersage der mittleren Immobilienpreise. Sie möchten vielleicht sicherstellen, dass der Testdatensatz  repräsentativ für die verschiedenen Einkommenskategorien des gesamten Datensatzes ist. Da das Medianeinkommen ein fortlaufendes numerisches Attribut ist, müssen Sie zunächst ein Einkommenskategorie-Attribut anlegen.\n",
    "\n",
    "Der folgende Code erzeugt ein **Einkommenskategorie-Attribut**, indem das Medianeinkommen durch 1,5 geteilt wird (um die Anzahl der Einkommensategorien zu begrenzen) und mit Hilfe von Obergrenzen aufgerundet wird (um diskrete Kategorien zu erhalten), und dann alle Kategorien größer als 5 in Kategorie 5 zusammengefasst werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0    0.350581\n",
       "2.0    0.318847\n",
       "4.0    0.176308\n",
       "5.0    0.114438\n",
       "1.0    0.039826\n",
       "Name: income_cat, dtype: float64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "housing = load_housing_data()\n",
    "# Erstellung income category Attribut mit fünf Kategorien\n",
    "housing[\"income_cat\"] = np.ceil(housing[\"median_income\"] / 1.5)\n",
    "housing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True)\n",
    "\n",
    "# Basierend auf dem Kategorie-Attribut wird nun eine stratifizierte Stichprobe gezogen\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "\n",
    "housing[\"income_cat\"].value_counts() / len(housing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
