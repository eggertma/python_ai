{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning von Modellen\n",
    "Nehmen wir an, Sie haben jetzt eine Liste mit vielversprechenden Modellen in der engeren Wahl. Sie müssen sie nun noch verfeinern. Sehen wir uns einige Möglichkeiten an, wie Sie das tun können."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search\n",
    "Eine Möglichkeit ist, mit den Hyperparametern (Konfigurationsparamter der ML-Algorithmen) manuell zu spielen, bis man eine hinreichend gute Kombination von Hyperparameterwerten findet. Das wäre sehr mühsam und Sie haben vielleicht nicht die Zeit, viele Kombinationen auszuprobieren. Stattdessen sollten Sie Scikit-Learn's **GridSearchCV** für die Suche nach geeigneten Hyperparametern einsetzen. Sie brauchen der Funktion nur zu sagen, mit welchen Hyperparametern sie experimentieren soll und welche Werte ausprobiert werden wollen. GridSearchCV wird dann alle möglichen Kombinationen von Hyperparameterwerten unter Verwendung von Kreuzvalidierung bewerten. Der folgende Code sucht z.B. nach der besten Kombination von Hyperparameterwerten für den RandomForestRegressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RandomForestRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-78f750f4b83f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m ]\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mforest_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n\u001b[0;32m      9\u001b[0m scoring='neg_mean_squared_error')\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RandomForestRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Array mit allen Paramtern, die im Rahmen des Trainings verändert werden sollen\n",
    "param_grid = [\n",
    "    {'n_estimators': [3, 10, 30],'max_features': [2, 4, 6, 8]},\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "]\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "grid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n",
    "scoring='neg_mean_squared_error')\n",
    "grid_search.fit(housing_prepared, housing_labels)\n",
    "\n",
    "# Ausgabe der besten Paramter:\n",
    "grid_search.best_params_\n",
    "\n",
    "grid_search.best_estimator_\n",
    "\n",
    "# Alle Scores der Leistungsmessungen\n",
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durch GridSearchCV werden unterschiedliche Paramter für den RandomForest-Algorithmus ausprobiert. Im ersten Schritt werden alle Kombinationen aus n_estimators und max_features (12 Kombinationen) ausprobiert. In einem zweiten Durchgang werden dann nochmal ohne bootstrapping zwei n_estiamtors mit drei max_features ausprobiert. Zusätzlich wird 5-fache Crossvalidierung ausprobiert. In Summe werden also 12+6 = 18 Kombinationen in 5 facher Cross-Validierung ausprobiert. Das macht in Summe 90 Trainingsrunden, um die besten Paramter zu ermitteln. Das dauert seine Zeit und sollte nur auf einem leistungsstarken Rechner oder über Nacht ausgeführt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zufällige Suche\n",
    "Der Ansatz der Rastersuche (GridSearchCV) ist in Ordnung, wenn Sie relativ wenige Kombinationen ausprobieren,\n",
    "wie im vorherigen Beispiel. Aber wenn der Hyperparameter-Suchraum groß ist, sollte man stattdessen **RandomizedSearchCV** verwenden. Diese Klasse arbeitet auf die gleiche Weise wie die Klasse GridSearchCV, aber anstatt alle möglichen Kombinationen auszuprobiere, bewertet sie eine bestimmte Anzahl von Zufallskombinationen durch Auswahl von zufälligen Werten für jeden Hyperparameter bei jeder Iteration. Dieser Ansatz hat zwei Hauptvorteile: \n",
    "- Wenn Sie die randomisierte Suche z.B. über 1.000 Iterationen laufen lassen, wird dieser Ansatz 1.000 verschiedene Werte für jeden Hyperparameter untersuchen (statt nur ein paar Werte pro Hyperparameter mit dem Grid-Search-Ansatz).\n",
    "- Sie haben mehr Kontrolle über das Rechenbudget, das Sie den Hyperparametern zuweisen wollen. Sie können einfach die Anzahl der Iterationen festlegen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation des Modells auf dem Testdatensatz\n",
    "Nachdem Sie eine Zeit lang an Ihren Modellen gefeilt haben, verfügen Sie letztlich über ein Modell, das ausreichend gut funktioniert. Jetzt ist es an der Zeit, das endgültige Modell auf dem Testset zu evaluieren. Es gibt nichts Besonderes an diesem Prozess; holen Sie einfach die Prädiktoren und die Labels aus Ihrem Testsatz, führen Sie Ihre Datentransformationspipeline (full_pipeline) aus, um die Daten zu transformieren (rufen Sie transform() auf, nicht fit_transform()!), und evaluieren Sie das endgültige Modell auf dem Testsatz. Der folgende Code ist aus Performance-gründen im Notebook nicht in vertretbarer Zeit ausführbar. Es gibt aber eine separate Übungsaufgabe zu Mashine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hohe das Modell mit den besten Hyperparametern\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "#Trenne den Testdatensatz in Faktoren (x) und Labels (y)\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "#Transforamtion der Testdaten (z.B. Behandlung von kategoriellen Werten)\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "\n",
    "# Vorhersageberechnung\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "# Bewertung der Leistungsfähigkeit bzw. Prognosegüte des Modells\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
