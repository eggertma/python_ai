{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vorbereitung der Daten für maschinelles Lernen\n",
    "Es ist an der Zeit, die Daten für Algorithmen des maschinellen Lernens vorzubereiten. Aus mehreren guten Gründen sollten Sie für die Datenvorbereitung Funktionen schreiben:\n",
    "- Dadurch können Sie die Transformationen leicht auf jedem beliebigen Datensatz reproduzieren (z.B, wenn Sie das nächste Mal einen neuen Datensatz erhalten)\n",
    "- Sie werden nach und nach eine Bibliothek von Transformationsfunktionen aufbauen, die Sie wiederverwenden können in zukünftigen Projekten\n",
    "- Sie können diese Funktionen in Ihrem Produktivsystem verwenden, um die neuen Daten zu transformieren, bevor Sie Ihren Algorithmus damit versorgen\n",
    "- Auf diese Weise können Sie einfach verschiedene Transformationen ausprobieren und sehen, welche Kombination von Transformationen am besten funktioniert\n",
    "\n",
    "Wir beginnen mit einem frischen Datensatz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Datenbeschaffung / Einlesen einer csv Datei wie in Abschnitt 2 beschrieben\n",
    "def load_housing_data():\n",
    "    csv_path = os.path.join(\"datasets/housing/housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "housing = load_housing_data()\n",
    "# Erstellung income category Attribut mit fünf Kategorien\n",
    "housing[\"income_cat\"] = np.ceil(housing[\"median_income\"] / 1.5)\n",
    "housing[\"income_cat\"].where(housing[\"income_cat\"] < 5, 5.0, inplace=True)\n",
    "\n",
    "# Basierend auf dem Kategorie-Attribut wird nun eine stratifizierte Stichprobe gezogen\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(housing, housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als erstes erstellen wir zwei Datenobjekte für das Trainieren des Algorithmus. Ein Datenobjekt enthält alle determinierenden Attribute (housing), exklusive des Lables bzw. des Zielattributes. Das zweite Datenobjekt soll ausschließlich die Zielattribute, in unserem Fall also die Hauswerte, enthalten. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"median_house_value\",axis=1)\n",
    "\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "Die meisten Algorithmen des maschinellen Lernens können nicht mit fehlenden Werten arbeiten, also lassen Sie uns ein paar Funktionen erstellen, um dieses Problem zu beheben. <br>\n",
    "Aufgabe 1: Lassen Sie sich nochmal eine Übersicht über die housing-Datensätze anzeigen. <br>\n",
    "Aufgabe 2: Wie erkennen Sie, ob ein Attribut fehlende Werte besitzt? Welches Attribut hat fehlende Werte?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 16512 entries, 17606 to 15775\n",
      "Data columns (total 10 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   longitude           16512 non-null  float64\n",
      " 1   latitude            16512 non-null  float64\n",
      " 2   housing_median_age  16512 non-null  float64\n",
      " 3   total_rooms         16512 non-null  float64\n",
      " 4   total_bedrooms      16354 non-null  float64\n",
      " 5   population          16512 non-null  float64\n",
      " 6   households          16512 non-null  float64\n",
      " 7   median_income       16512 non-null  float64\n",
      " 8   ocean_proximity     16512 non-null  object \n",
      " 9   income_cat          16512 non-null  float64\n",
      "dtypes: float64(9), object(1)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie haben drei Möglichkeiten mit fehlenden Werten umzugehen:\n",
    "- Sie können die entsprechenden Bezirke entfernen (Option 1)\n",
    "- Sie können das gesamte Attribut entfernen (Option 2)\n",
    "- Sie können die Werte manuell belegen (Null, den Mittelwert, den Median usw.) (Option 3)\n",
    "\n",
    "Dies können Sie leicht mit den Methoden **dropna(), drop() und fillna() von DataFrame** erreichen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17606     351.0\n",
       "18632     108.0\n",
       "14650     471.0\n",
       "3230      371.0\n",
       "3555     1525.0\n",
       "          ...  \n",
       "6563      236.0\n",
       "12053     294.0\n",
       "13908     872.0\n",
       "11159     380.0\n",
       "15775     682.0\n",
       "Name: total_bedrooms, Length: 16512, dtype: float64"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Option 1\n",
    "#housing.dropna(subset=[\"total_bedrooms\"]) \n",
    "# Option 2\n",
    "#housing.drop(\"total_bedrooms\", axis=1) \n",
    "# Option 3\n",
    "median = housing[\"total_bedrooms\"].median()\n",
    "housing[\"total_bedrooms\"].fillna(median) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine weitere Möglichkeit, fehlende Werte zu ersetzen stellt die Imputer-Methode bereit. Details erhalten Sie hier: https://scikit-learn.org/0.16/modules/generated/sklearn.preprocessing.Imputer.html.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behandlung von Text und Kategorie-Attributen\n",
    "Eben haben wir das kategorische Attribut Ozean_Nähe ausgelassen, weil es ein Textattribut ist, so dass wir seinen Median nicht berechnen können. Die meisten Algorithmen des maschinellen Lernens arbeiten jedoch lieber mit Zahlen. Also lassen Sie uns Textattribute in Zahlen umwandeln. Scikit-Learn stellt für diese Aufgabe einen Transformator namens **LabelEncoder** zur Verfügung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 4, ..., 1, 0, 3])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "housing_cat = housing[\"ocean_proximity\"]\n",
    "housing_cat_encoded = encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das ist besser: Jetzt können wir diese numerischen Daten in jedem ML-Algorithmus verwenden. Aber was bedeuten die Zahlen? <br>Aufgabe 3: Wie bekommen Sie die Zuordnungen der Bezeichnungen zu den Zahlen wiedergegeben? (Hinweis: Der Encoder arbeitet mit dem Attribut classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier sollte Ihr Code rein, um die Bezeichnungszuordnungen zu bekommen\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das Problem mit nummerischen Werten in maschinellem Lernen ist, dass eine gewisse Nähe der Werte in Abhängigkeit zu ihrer zhalenmäßigen Distanz angenommen wird. In unserem Fall würde der Algorithmus also annehmen, dass zwischen den Werten 0 und 1 eine gewisse Ähnlichkeit besteht, was in unserem Beispiel falsch ist. Um diesen Effekt zu vermeiden, bietet Scikit-Learn den **OneHotEncoder** an. Die Methode erstellt ein binäres Attribut (nur 0 oder 1 Werte möglich) für jede Attributsausprägung. Also beispielsweise das Attribut \"INLAND\" mit dem Wert 0, wenn der Ortsbereich sich am Meer befindet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder2 = OneHotEncoder()\n",
    "\n",
    "# Der Output von fit_transform ist ein Objekt vom Typ Sparse matrix mit binären Werten. \n",
    "housing_cat_1hot = encoder2.fit_transform(housing_cat_encoded.reshape(-1,1))\n",
    "\n",
    "# Umwandlung einer dünn besetzten Matrix , in der nur die 1en gespeiochert werden in eine dicht \n",
    "# besetzte Matrix mit 0en un 1en\n",
    "housing_cat_1hot = housing_cat_1hot.toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigene Transformatoren bauen\n",
    "Obwohl Scikit-Learn viele nützliche Transformatoren bietet, müssen Sie manchmal auch Ihre eigenen Transformatoren schreiben. Zum Beispiel für Aufgaben wie benutzerdefinierte Bereinigungsoperationen oder das Kombinieren bestimmter Attribute. Dafür müssen Sie eine Klasse erstellen und drei Methoden implementieren: \n",
    "- fit() (selbstrückkehrend),\n",
    "- transform() und \n",
    "- fit_transform(). \n",
    "\n",
    "Hier ist zum Beispiel eine kleine Transformatorklasse, die Folgendes hinzufügt die kombinierten Attribute, die wir vorhin besprochen haben:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "rooms_ix, bedrooms_ix, population_ix, household_ix = 3, 4, 5, 6\n",
    "\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, add_bedrooms_per_room = True): # no *args or **kargs\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing else to do\n",
    "\n",
    "    # Da die Methode nur die Werte übergeben bekommt. muss auf das Array mit Spaltennummern (rooms_ix etc.) zugegriffen werden\n",
    "    def transform(self, X, y=None):\n",
    "        rooms_per_household = X[:, rooms_ix] / X[:, household_ix]\n",
    "        population_per_household = X[:, population_ix] / X[:, household_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
    "            return np.c_[X, rooms_per_household, population_per_household,bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X, rooms_per_household, population_per_household]\n",
    "\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "# Das Objekt bekommt über housing.values nur die Werte übergeben. Die Attributsbezeichnungen nicht.\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Beispiel hat der Transformator einen Hyperparameter, *add_bedrooms_per_room*, der standardmäßig auf True gesetzt ist (es ist oft hilfreich, sinnvolle Vorgaben zu machen). Mit diesem Hyperparameter können Sie leicht herausfinden, ob das Hinzufügen dieses Attributs dem ML-Algorithmus hilft oder nicht. Allgemeiner gesagt, Sie können einen Hyperparameter hinzufügen, um jeden Datenvorbereitungsschritt zu steuern, bei dem Sie sich nicht 100%ig sicher sind. Je mehr Sie diese Datenvorbereitungsschritte automatisieren, desto mehr Kombinationen können Sie automatisch ausprobieren, wodurch es viel wahrscheinlicher wird, dass Sie eine treffende Kombination finden (und Sie viel Zeit sparen).\n",
    "\n",
    "Aufgabe 4: Lassen Sie sich die Werte vom housing-Datensatz anzeigen und vergleichen Sie die Ausgabe mit den Werten in housing_extra_attribs. Was fällt Ihnen auf?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datenskallierung\n",
    "Eine der wichtigsten Transformationen, die Sie auf Ihre Daten anwenden müssen, ist die Daten oder *Feature-Skalierung*. Von wenigen Ausnahmen abgesehen, schneiden die Algorithmen des maschinellen Lernens nicht gut ab, wenn die eingegebenen numerischen Attribute sehr unterschiedliche Skalen haben. Dies ist der Fall für die housing-Daten: Die Gesamtzahl der Räume reicht von etwa 6 bis 39.320, während der Median der Einkommen nur zwischen 0 und 15 liegt. Beachten Sie, dass die Skalierung der Zielwerte im Allgemeinen nicht erforderlich ist.\n",
    "Es gibt zwei gängige Methoden, um alle Attribute auf die gleiche Skala zu bringen: **min-max Skalierung** und **Standardisierung**.\n",
    "\n",
    "Die **Min-Max-Skalierung** (viele nennen dies Normalisierung) ist recht einfach: Die Werte sind verschoben und neu skaliert, so dass sie schließlich von 0 bis 1 reichen. Wir tun dies, indem wir den Min-Wert substrahieren und durch den Max-Wert minus den Min.-Wert dividieren. Scikit-Learn bietet eine Transformator dafür: den **MinMaxScaler**. Er hat einen Feature_Bereichs-Hyperparameter, mit dem Sie den Bereich ändern können, wenn Sie aus irgendeinem Grund nicht den Bereich von 0-1 haben wollen. \n",
    "\n",
    "Die **Standardisierung** ist ganz anders: Zuerst wird der Mittelwert substrahiert (standardisierte Werte haben immer einen Mittelwert von Null), und dann wird durch die Varianz geteilt, so dass die resultierende Verteilung eine Einheitsvarianz aufweist. Anders als bei der Min-Max-Skalierung werden bei der Standardisierung nicht Werte an einen bestimmten Bereich gebunden, was für einige Algorithmen ein Problem darstellen kann (z.B, neuronale Netze erwarten oft einen Eingabewert im Bereich von 0 bis 1). Allerdings ist die Standardisierung viel weniger von Ausreißern betroffen. Nehmen wir beispielsweise an, ein Distrikt hätte ein durchschnittliches Einkommen gleich 100 (versehentlich). Die Min-Max-Skalierung würde dann alle anderen vernichten. Werte von 0-15 bis hinunter zu 0-0,15, während die Standardisierung nicht wesentlich beeinträchtigt würde. Scikit-Learn stellt einen Transformator namens **StandardScaler** für die Normung zur Verfügung.\n",
    "\n",
    "**Wichtig** \n",
    "Bitte beachten Sie, dass Skalierungen **ausschließlich auf den Trainingsdaten** angewendet werden, nicht auf den Testdaten oder den gesamten Grunddaten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations Pipelines\n",
    "Wie wir nun wissen, müssen zahlreiche Transformationen auf den Ursprungsdaten ausgeführt werden, damit sie in einem ML-Algorithmus genutzt werden können. Glücklicherweise bietet Scikit-Learn auch hierfür eine Klasse, die die Erstellung solcher Transformations Pipelines (data pipeline) unterstützt: sklearn.pipeline. Das folgende Beispiel verdeutlicht die Funktionsweise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.15604281,  0.77194962,  0.74333089, ..., -0.31205452,\n",
       "        -0.08649871,  0.15531753],\n",
       "       [-1.17602483,  0.6596948 , -1.1653172 , ...,  0.21768338,\n",
       "        -0.03353391, -0.83628902],\n",
       "       [ 1.18684903, -1.34218285,  0.18664186, ..., -0.46531516,\n",
       "        -0.09240499,  0.4222004 ],\n",
       "       ...,\n",
       "       [ 1.58648943, -0.72478134, -1.56295222, ...,  0.3469342 ,\n",
       "        -0.03055414, -0.52177644],\n",
       "       [ 0.78221312, -0.85106801,  0.18664186, ...,  0.02499488,\n",
       "         0.06150916, -0.30340741],\n",
       "       [-1.43579109,  0.99645926,  1.85670895, ..., -0.22852947,\n",
       "        -0.09586294,  0.10180567]])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Erstellen eines Dataframes ohne kategorielle Attribute\n",
    "housing_num = housing.drop(\"ocean_proximity\", axis=1)\n",
    "\n",
    "# SimpleImputer ersetzt alle fehlenden Werte mit Ersatzwerten, in diesem Fall dem Median aller Werte der Spalte\n",
    "# CombinedAttributesAdder ist die Transformationsklasse, die Attribute zusammensetzt (Bsp Anzahl Schlafzimmer / Anzahl Räume)\n",
    "# StandardScaler skalliert alle Daten\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('attribs_adder', CombinedAttributesAdder()),\n",
    "    ('std_scaler', StandardScaler())])\n",
    "\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "housing_num_tr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe 5:** Lassen Sie sich die Inhalte von housing_num und von housing_num_tr ausgeben und vergleichen Sie den Output. Was fällt Ihnen auf?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ihr Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sie haben jetzt eine Pipeline für numerische Werte. Wir haben auch bereits einen Weg kennen gelernt, um kategorielle Attribute in binäre Attribute umzuwandeln. Eine weitere Möglichkeit bietet der *LabelBinarizer*, den wir gleich kennen lernen werden. \n",
    "\n",
    "Die zentrale Frage aber ist: *Wie kann man diese Transformationen zu einer einzigen Pipeline zusammenführen?* Scikit-Learn bietet hierfür eine **FeatureUnion-Klasse** an. Sie geben ihr eine Liste von Transformatoren (die auch ganze Transformator-Pipelines sein können). Wenn seine transform() Methode aufgerufen wird, führt sie die transform()-Methode jedes Transformators parallel aus, wartet auf ihre Ausgabe, verkettet sie dann und gibt das Ergebnis zurück Eine vollständige Pipeline-Behandlung sowohl numerischer als auch kategorialer Attribute könnte so aussehen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.base import BaseEstimator , TransformerMixin\n",
    "\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "# Klasse für die Auswahl nummerischer und kategorieller Spalten\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values\n",
    "\n",
    "# Pipeline für ide Verarbeitung nummerischer Attribute\n",
    "num_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(num_attribs)),\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('attribs_adder', CombinedAttributesAdder()),\n",
    "    ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "\n",
    "# Pipeline für die Verarbeitung kategorieller Attribute\n",
    "cat_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(cat_attribs)),\n",
    "    ('label_binarizer', OneHotEncoder()),\n",
    "    ])\n",
    "\n",
    "# Zusammensetzen der Teil-Pipelines\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "    (\"num_pipeline\", num_pipeline),\n",
    "    (\"cat_pipeline\", cat_pipeline),\n",
    "    ])\n",
    "\n",
    "# Bis hierher arbeitet die pipeline noch nicht mit echten Daten. Sie verfügt nur über das Wissen über die Attribute und der \n",
    "# Transformationsfunktionen. Erst jetzt werden der Pipeline echte housing-Daten übergeben:\n",
    "housing_prepared = full_pipeline.fit_transform(housing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
